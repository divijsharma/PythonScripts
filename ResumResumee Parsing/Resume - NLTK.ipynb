{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pending\n",
    "1. Processing docx file<br>\n",
    "   \n",
    "#### Configuration file\n",
    "1. Directory where resumes are stored<br>\n",
    "2. The extra words file<br>\n",
    "3. Temporary resume file<br>\n",
    "4. Comprehensive dictionary file<br>\n",
    "5. Directory where JDs are stored<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brown dictionary\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "# For punctuation marks\n",
    "import string\n",
    "# For reading docx file\n",
    "try:\n",
    "    from xml.etree.cElementTree import XML\n",
    "except ImportError:\n",
    "    from xml.etree.ElementTree import XML\n",
    "import zipfile\n",
    "# For reading pdf file\n",
    "import PyPDF2\n",
    "# For regular expression\n",
    "import re\n",
    "from docx import Document\n",
    "# For similarity in document comparison\n",
    "import gensim\n",
    "# For reading directory\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the config file name in main directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = './Resume/'\n",
    "config_file = [file for file in os.listdir(working_dir) if os.path.isfile(os.path.join(working_dir,file)) and 'config_' in file]\n",
    "config_file_path = working_dir + config_file[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the configuration file that has \n",
    "1. Resume Directory\n",
    "2. Path of Extra dictionary\n",
    "3. Path of temporary resume file\n",
    "4. Path of my clean dictionary\n",
    "5. JD Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the configuration file that has path of resume\n",
    "config_file = open(file=config_file_path, mode='r')\n",
    "for line in config_file:\n",
    "    if line[:7] == 'resdir=':\n",
    "        resume_dir = line[7:]\n",
    "        # Removing new line character from the end\n",
    "        resume_dir = re.sub(r'[\\n\\r]+$', '', resume_dir)\n",
    "        \n",
    "    if line[:7] == 'exdict=':\n",
    "        extra_dict_path = line[7:]\n",
    "        # Removing new line character from the end\n",
    "        extra_dict_path = re.sub(r'[\\n\\r]+$', '', extra_dict_path)\n",
    "\n",
    "    if line[:7] == 'tempre=':\n",
    "        temp_resume_txt_path = line[7:]\n",
    "        # Removing new line character from the end\n",
    "        temp_resume_txt_path = re.sub(r'[\\n\\r]+$', '', temp_resume_txt_path)\n",
    "\n",
    "    if line[:7] == 'mydict=':\n",
    "        my_dict_path = line[7:]\n",
    "        # Removing new line character from the end\n",
    "        my_dict_path = re.sub(r'[\\n\\r]+$', '', my_dict_path)\n",
    "\n",
    "    if line[:7] == 'jd_dir=':\n",
    "        jd_dir = line[7:]\n",
    "        # Removing new line character from the end\n",
    "        jd_dir = re.sub(r'[\\n\\r]+$', '', jd_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the clean dictionary I created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "http = urllib3.PoolManager()\n",
    "response = http.request('GET' my_dict_path)\n",
    "my_dict = set(response.data.decode('utf-8').split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read extra words files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file=extra_dict_path, mode='r') as extra_dict_file:\n",
    "    extra_dict = set(extra_dict_file.read().split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the inbuilt brown dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_dict = set(brown.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Union of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_good_word_list = extra_dict | brown_dict | my_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Resume and JD path list\n",
    "##### Resumes have to start with resume_ and Job Descriptions have to start with JD_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_path_list = [resume_dir + file for file in os.listdir(resume_dir) if os.path.isfile(os.path.join(resume_dir,file)) and 'resume_' in file]\n",
    "jd_path_list = [jd_dir + file for file in os.listdir(jd_dir) if os.path.isfile(os.path.join(jd_dir,file)) and 'JD_' in file]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spelling Mistake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spelling_mistakes = {}\n",
    "counter = 1\n",
    "# Loop through all the resume files\n",
    "for path in resume_path_list:\n",
    "    index_of_last_dot = path.rfind(\".\")\n",
    "    resume_type = path[index_of_last_dot+1:]\n",
    "    \n",
    "    # Logic to read the file based on the extension\n",
    "    if resume_type == \"txt\":\n",
    "        with open(file=resume_path, mode='r') as resume_file:\n",
    "            resume_words = resume_file.read()\n",
    "            \n",
    "    if resume_type == \"pdf\":\n",
    "        with open(file=resume_path, mode='rb') as pdf_resume, open(file=temp_resume_txt_path, mode='w') as temp_txt_resume:\n",
    "            read_pdf = PyPDF2.PdfFileReader(pdf_resume)\n",
    "            total_pages = read_pdf.getNumPages()\n",
    "            \n",
    "            for page_number in range(total_pages):\n",
    "                page = read_pdf.getPage(page_number)\n",
    "                page_content = page.extractText()\n",
    "                temp_txt_resume.write(page_content)\n",
    "\n",
    "        with open(file=temp_resume_txt_path, mode='r') as temp_txt_resume:\n",
    "            resume_words = temp_txt_resume.read()\n",
    "        \n",
    "    # Cleaning the data\n",
    "    # Removing numbers\n",
    "    resume_words = re.sub(r'\\d+', '', resume_words)\n",
    "    # Removing punctuations\n",
    "    resume_words = re.sub(r'[^a-zA-Z0-9\\s]', ' ', resume_words)\n",
    "    # Removing extra new lines and tab\n",
    "    resume_words = re.sub(r\"[\\n\\t]*\", \"\", resume_words)\n",
    "    # Convert all the words that have only first letter as uppercase to all lower case example - \"Test\" will be converted to\n",
    "    # \"test\". \"TEST\" will not be converted\n",
    "    for w in re.findall(r'[A-Z][a-z]+', resume_words):\n",
    "        resume_words = resume_words.replace(w, w.lower())\n",
    "\n",
    "    # Remove Stopwords\n",
    "    tokenized_words = word_tokenize(resume_words)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokenized_words = [w for w in tokenized_words if not w in stop_words]\n",
    "\n",
    "    # Logic to find out the spelling mistake\n",
    "    wrong_spelling_resume = []\n",
    "    wrong_spelling_resume = [word for word in tokenized_words if not word in final_good_word_list]\n",
    "\n",
    "    # Eliminating duplicates\n",
    "    unique_wrong_spelling_resume = []\n",
    "    [unique_wrong_spelling_resume.append(word) for word in wrong_spelling_resume if word not in unique_wrong_spelling_resume]\n",
    "    \n",
    "    # Writing spelling mistake in a dictionary\n",
    "    spelling_mistakes[str(counter)] = unique_wrong_spelling_resume\n",
    "    # Increase the counter for next resume\n",
    "    counter = counter + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the spelling errors counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for counter in range(len(spelling_mistakes)):\n",
    "    print(\"Number of spelling mistake in resume\", counter+1, \"=\", len(spelling_mistakes[str(counter+1)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Spelling Mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for counter in range(len(spelling_mistakes)):\n",
    "    print(\"Spelling mistakes in resume\", counter+1, \"are as follows\")\n",
    "    print(\"*\" *100)\n",
    "    print(spelling_mistakes[str(counter+1)])\n",
    "    print(\"*\" *100, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Matching - Document Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read all Job description files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jd_combine = []\n",
    "for path in jd_path_list:\n",
    "    with open(file=path, mode='r') as jd_file:\n",
    "        jd = jd_file.read()\n",
    "    jd_combine.append(jd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Job Description docs\n",
    "1. Tokenize\n",
    "2. Create dictionary\n",
    "3. Corpus\n",
    "4. tf_idf model\n",
    "5. Similarity object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Tokenize\n",
    "jd_docs = [[w.lower() for w in word_tokenize(text)]\n",
    "          for text in jd_combine]\n",
    "\n",
    "# Step 2 - Create dictionary from list of documents. A dictionary maps every word to a number. Dictionary will have only unique\n",
    "# words. \n",
    "jd_dictionary = gensim.corpora.Dictionary(jd_docs)\n",
    "\n",
    "# Step 3 - Create corpus from the dictionary. A corpus is a list of Bag Of Words (bow). A bow representaiton for a document\n",
    "# just lists the number of times each word occurs in the document.\n",
    "jd_corpus = [jd_dictionary.doc2bow(jd_doc) for jd_doc in jd_docs]\n",
    "\n",
    "# Step 4 - Create tf-idf model for corpus\n",
    "jd_tf_idf = gensim.models.TfidfModel(jd_corpus)\n",
    "\n",
    "# Step 5 - Create a similarity measure object in tf-idf space. Term frequency is how often the word shows up in the document \n",
    "# and inverse document fequency scales the value by how rare the word is in the corpus.\n",
    "jd_sims = gensim.similarities.Similarity(working_dir, jd_tf_idf[jd_corpus], num_features=len(jd_dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through the resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jd_resume_similarity = {}\n",
    "counter = 1\n",
    "# Loop through all the resume files\n",
    "for path in resume_path_list:\n",
    "    index_of_last_dot = path.rfind(\".\")\n",
    "    resume_type = path[index_of_last_dot+1:]\n",
    "\n",
    "    # Logic to read the file based on the extension\n",
    "    if resume_type == \"txt\":\n",
    "        with open(file=resume_path, mode='r') as resume_file:\n",
    "            resume_words = resume_file.read()\n",
    "            \n",
    "    if resume_type == \"pdf\":\n",
    "        with open(file=path, mode='rb') as pdf_resume, open(file=temp_resume_txt_path, mode='w') as temp_txt_resume:\n",
    "            read_pdf = PyPDF2.PdfFileReader(pdf_resume)\n",
    "            total_pages = read_pdf.getNumPages()\n",
    "            \n",
    "            for page_number in range(total_pages):\n",
    "                page = read_pdf.getPage(page_number)\n",
    "                page_content = page.extractText()\n",
    "                temp_txt_resume.write(page_content)\n",
    "\n",
    "        with open(file=temp_resume_txt_path, mode='r') as temp_txt_resume:\n",
    "            resume_words = temp_txt_resume.read()\n",
    "\n",
    "    # Tokenize Resume\n",
    "    resume_doc = [w.lower() for w in word_tokenize(resume_words)]\n",
    "    # Create BOW from dictionary\n",
    "    resume_doc_bow = jd_dictionary.doc2bow(resume_doc)\n",
    "    # Create tf-idf\n",
    "    resume_doc_tf_idf = jd_tf_idf[resume_doc_bow]\n",
    "    # Writing Similarity in dictionary\n",
    "    jd_resume_similarity[str(counter)] = jd_sims[resume_doc_tf_idf]\n",
    "    # Increase the counter for next resume\n",
    "    counter = counter + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the similarity %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for counter in range(len(jd_resume_similarity)):\n",
    "    print(\"Similarity percent of Resume\", counter+1, \"with JDs provided are\")\n",
    "    for i in range(len(jd_resume_similarity[str(counter+1)])):\n",
    "        print(round(jd_resume_similarity[str(counter+1)][i]*100, 2))\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
