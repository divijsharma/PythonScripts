{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ~Things to do~\n",
    "1. ~Scrape the website~\n",
    "2. ~Get the list of files of Production data~\n",
    "3. ~Get the list of urls from that section~\n",
    "4. ~Go to each url and get the list of .gz files~\n",
    "5. ~Download each .gz file - folder name is different for every one of them \\<td\\> name~\n",
    "\n",
    "https://www.rrc.state.tx.us/about-us/resource-center/research/data-sets-available-for-download/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "from ftplib import FTP\n",
    "import os\n",
    "import urllib.request as req\n",
    "import shutil\n",
    "from contextlib import closing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for Jupyter notebook only\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the page in memory first\n",
    "url = 'https://www.rrc.state.tx.us/about-us/resource-center/research/data-sets-available-for-download/'\n",
    "page = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_data = pd.DataFrame()\n",
    "if page.status_code == 200:\n",
    "    soup = bs(page.content, 'lxml')\n",
    "    # Upon inspection of the page - data to be extracted is in id=\"production-data-table\" \n",
    "    tables = soup.select('table') # Reading all the table tags\n",
    "    for table in tables:\n",
    "        table_head = table.find('thead') # Get the head tag \n",
    "        header_rows = table_head.find_all('tr') # Get the tr \n",
    "        for header_row in header_rows:\n",
    "        # Till this point all the code is custom for the website. Depending on the website source we have to write the \n",
    "        # code for traversing through the tags.\n",
    "            if header_row.next_element.attrs.get('id') == 'production-data-table':\n",
    "            # Read the body\n",
    "                table_body = table.find('tbody')\n",
    "                row_index = 0\n",
    "                for body_row in table_body.find_all('tr'):\n",
    "                    col_index = 0\n",
    "                    body_cols = body_row.find_all('td')\n",
    "                    for col in body_cols:\n",
    "                        if col_index == 0:\n",
    "                            table_data.loc[row_index, col_index] = col.get_text()\n",
    "                        if col_index == 1:\n",
    "                            url = col.find('a')\n",
    "                            table_data.loc[row_index, col_index] = str(url)\n",
    "                        if col_index == 2:\n",
    "                            url = col.find('a')\n",
    "                            table_data.loc[row_index, col_index] = str(url)\n",
    "                        col_index += 1\n",
    "                    row_index += 1\n",
    "else:\n",
    "    print('Error in reading page')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column names of the newly formed dataframe\n",
    "column_names = ['dataset_name', 'data_url_format', 'data_url_manual']\n",
    "table_data.columns = column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the URL from the column\n",
    "table_data['data_url'] = table_data.data_url_format.str.extract('\\\"(.*)\\\"', expand = True)\n",
    "table_data['data_desc_url'] = 'https://www.rrc.state.tx.us' + \\\n",
    "                                                        table_data.data_url_manual.str.extract('\\\"(.*)\\\"', expand = True)\n",
    "table_data['data_format'] = table_data.data_url_format.str.extract('>(.*)<', expand = True)\n",
    "table_data['data_desc_format'] = table_data.data_url_manual.str.extract('>(.*)<', expand = True)\n",
    "\n",
    "# Dropping the column\n",
    "table_data.drop(columns = ['data_url_format', 'data_url_manual'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ~Things to Do~\n",
    "1. ~Clean dataset_name column~\n",
    "    - ~Replace ( with spaces~\n",
    "    - ~Replace ) with empty string~\n",
    "    - ~Replace / with space~\n",
    "    - ~Replace - with empty string~\n",
    "    - ~Remove puntuations~\n",
    "    - ~Remove all alphabet + number combination (6E, 7B etc)~\n",
    "    - ~Remove numbers~\n",
    "2. ~Remove all the rows that have NaN~\n",
    "3. ~Create a new dataframe with the unique values of dataset_name column~\n",
    "4. ~From data_url column~\n",
    "    - ~Extract ftp://ftpe.rrc.texas.gov - from begining till the last /~\n",
    "    - ~Extract ftpe.rrc.texas.gov - between // and /~\n",
    "    - ~Extract folder name - after last /~\n",
    "    - ~Create new columns for all the above~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the below characters separately because of special consideration\n",
    "table_data.dataset_name = table_data.dataset_name.str.replace('(', ' ').str.replace(')', '').str.replace('/', ' ')\n",
    "# Replace charratcers like 8A, 6E etc\n",
    "table_data.dataset_name = table_data.dataset_name.str.replace(r'[0-9][a-zA-Z]', '')\n",
    "# Replace all numbers\n",
    "table_data.dataset_name = table_data.dataset_name.str.replace(r'\\W\\d{1,}', '')\n",
    "# Replace all punctuaitons\n",
    "table_data.dataset_name = table_data.dataset_name.str.replace(r'[^\\w\\s]', '')\n",
    "# Replace ' and '\n",
    "table_data.dataset_name = table_data.dataset_name.str.replace(' and', '')\n",
    "# Replace multiple consecutive whitespaces\n",
    "table_data.dataset_name = table_data.dataset_name.str.replace(r'\\s{1,}', ' ')\n",
    "# Remove the last space\n",
    "table_data.dataset_name = table_data.dataset_name.str.rstrip()\n",
    "# Remove all the rows that have NaN in  any row\n",
    "table_data = table_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping all the duplicate rows\n",
    "table_data = table_data.drop_duplicates(keep='last').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_data['ftpe_gov_url'] = table_data.data_url.str.extract('//(.*)/', expand = True)\n",
    "table_data['intial_ftp_url'] = table_data.data_url.str.extract('^(.*)/', expand = True)\n",
    "table_data['folder_name'] = table_data.data_url.str.rsplit('/').str[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_data['file_url_list'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to ftp page and getting the list of files from the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'230 Login successful.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'250 Directory successfully changed.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'230 Login successful.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'250 Directory successfully changed.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'230 Login successful.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'250 Directory successfully changed.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'230 Login successful.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'250 Directory successfully changed.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'230 Login successful.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'250 Directory successfully changed.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'230 Login successful.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'250 Directory successfully changed.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'230 Login successful.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'250 Directory successfully changed.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'230 Login successful.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'250 Directory successfully changed.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'230 Login successful.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'250 Directory successfully changed.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index in table_data.index:\n",
    "    with FTP(table_data.loc[index, 'ftpe_gov_url']) as ftp:\n",
    "        ftp.login()\n",
    "        ftp.cwd(table_data.loc[index, 'folder_name'])\n",
    "        data = ftp.nlst()\n",
    "\n",
    "#     path = table_data.loc[index, 'intial_ftp_url']\n",
    "    # Using table_data.loc[index, 'file_url_list'] = [path + lnk for lnk in data] to insert the list in a column \n",
    "    # an gives and error 'ValueError: Must have equal len keys and value when setting with an iterable'. Therefore,\n",
    "    # use .at. DataFrame.at accesses a single value for a row/column label pair.\n",
    "    table_data.at[index, 'file_url_list'] = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ~Things to do~\n",
    "1. ~Create the folder based on dataset_name~\n",
    "2. ~Download the files from the file_url_list~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = 'C:\\\\Users\\\\Admin\\\\Desktop\\\\Study\\\\NLP\\\\FTP_Files\\\\'\n",
    "for index in table_data.index:\n",
    "    download_folder = base_folder + table_data.loc[index, 'dataset_name']\n",
    "    try:\n",
    "        os.makedirs(download_folder)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    except OSError as e:\n",
    "        raise\n",
    "    \n",
    "    url_list = table_data.loc[index, 'file_url_list']\n",
    "    for lst_index in range(len(url_list)): \n",
    "        file = table_data.loc[index, 'data_url'] + '/' + url_list[lst_index]\n",
    "        download_file_name = download_folder + '\\\\' + url_list[lst_index]\n",
    "\n",
    "        with closing(req.urlopen(file)) as r:\n",
    "            with open(download_file_name, 'wb') as f:\n",
    "                shutil.copyfileobj(r, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
