{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Various methods to split the data in train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dsharma\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data, clean data and separate features and response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris() # In the inbuilt Iris dataset the features and response variables are already separated in iris.data\n",
    "# and iris.target\n",
    "adult_data = pd.read_csv('./Adult_Data/adult_data.csv', header = None)\n",
    "# 1. Rename columns\n",
    "adult_data.rename(columns={0: 'age',\n",
    "                           1: 'workclass',\n",
    "                           2: 'fnlwgt',\n",
    "                           3: 'education',\n",
    "                           4: 'education-num',\n",
    "                           5: 'marital-status',\n",
    "                           6: 'occupation',\n",
    "                           7: 'relationship',\n",
    "                           8: 'race',\n",
    "                           9: 'sex',\n",
    "                           10: 'capital-gain',\n",
    "                           11: 'capital-loss',\n",
    "                           12: 'hours-per-week',\n",
    "                           13: 'native-country',\n",
    "                           14: 'salary'}, inplace = True)\n",
    "# 2. Replace >50K with 1 and <=50K as 0. There are whitespaces in the column value that's why a simple replace would not work.\n",
    "# strip the whitespaces first\n",
    "adult_data['salary'] = adult_data['salary'].str.strip().replace({\">50K\": 1, \"<=50K\": 0})\n",
    "# 3. Separate features and response\n",
    "number_cols = adult_data.shape[1]\n",
    "adult_data_response = adult_data['salary'] # adult_data_response is a vector\n",
    "adult_data_features = adult_data.iloc[:, 0:number_cols-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random split based on percent of data\n",
    "The input data is simply divided into training and test dataset based on a predefined %. In the below code - training set will \n",
    "have 80% of the data and test set will have 20% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_data_features_train, adult_data_features_test, adult_data_response_train, adult_data_response_test = \\\n",
    "            train_test_split(adult_data_features, adult_data_response, test_size = 0.2, random_state = 4)\n",
    "    \n",
    "iris_features_train, iris_features_test, iris_response_train, iris_response_test = \\\n",
    "            train_test_split(iris.data, iris.target, test_size = 0.2, random_state = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADULT DATASET\n",
      "****************************************************************************************************\n",
      "Total number of rows in Adult dataset =  32561\n",
      "Total number of columns in Adult dataset =  15\n",
      "****************************************************************************************************\n",
      "Total number of rows in Features Train dataset =  26048\n",
      "Total number of columns in Features Train dataset =  14\n",
      "****************************************************************************************************\n",
      "Total number of rows in Features Test dataset =  6513\n",
      "Total number of columns in Features Test dataset =  14\n",
      "****************************************************************************************************\n",
      "Sum of rows Features Test and Train dataset =  32561\n",
      "****************************************************************************************************\n",
      "Total number of rows in Target Train dataset =  26048\n",
      "****************************************************************************************************\n",
      "Total number of rows in Target Test dataset =  6513\n",
      "****************************************************************************************************\n",
      "Sum of rows Target Test and Train dataset =  52096\n",
      "****************************************************************************************************\n",
      "The Target Train and Test sets is a vector so there is no .shape[1] value\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "print(\"ADULT DATASET\")\n",
    "print(\"*\" * 100)\n",
    "print(\"Total number of rows in Adult dataset = \", adult_data.shape[0])\n",
    "print(\"Total number of columns in Adult dataset = \", adult_data.shape[1])\n",
    "print(\"*\" * 100)\n",
    "print(\"Total number of rows in Features Train dataset = \", adult_data_features_train.shape[0])\n",
    "print(\"Total number of columns in Features Train dataset = \", adult_data_features_train.shape[1])\n",
    "print(\"*\" * 100)\n",
    "print(\"Total number of rows in Features Test dataset = \", adult_data_features_test.shape[0])\n",
    "print(\"Total number of columns in Features Test dataset = \", adult_data_features_test.shape[1])\n",
    "print(\"*\" * 100)\n",
    "print(\"Sum of rows Features Test and Train dataset = \", adult_data_features_train.shape[0] + adult_data_features_test.shape[0])\n",
    "print(\"*\" * 100)\n",
    "print(\"Total number of rows in Target Train dataset = \", adult_data_response_train.shape[0])\n",
    "print(\"*\" * 100)\n",
    "print(\"Total number of rows in Target Test dataset = \", adult_data_response_test.shape[0])\n",
    "print(\"*\" * 100)\n",
    "print(\"Sum of rows Target Test and Train dataset = \", adult_data_response_train.shape[0] + adult_data_response_train.shape[0])\n",
    "print(\"*\" * 100)\n",
    "print(\"The Target Train and Test sets is a vector so there is no .shape[1] value\")\n",
    "print(\"*\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IRIS DATASET\n",
      "****************************************************************************************************\n",
      "Total number of rows in Iris dataset =  150\n",
      "Total number of columns in Iris dataset =  4\n",
      "****************************************************************************************************\n",
      "Total number of rows in Features Train dataset =  120\n",
      "Total number of columns in Features Train dataset =  4\n",
      "****************************************************************************************************\n",
      "Total number of rows in Features Test dataset =  30\n",
      "Total number of columns in Features Test dataset =  4\n",
      "****************************************************************************************************\n",
      "Sum of rows Features Test and Train dataset =  150\n",
      "****************************************************************************************************\n",
      "Total number of rows in Target Train dataset =  120\n",
      "****************************************************************************************************\n",
      "Total number of rows in Target Test dataset =  30\n",
      "****************************************************************************************************\n",
      "Sum of rows Target Test and Train dataset =  150\n",
      "****************************************************************************************************\n",
      "The Target Train and Test sets is a vector so there is no .shape[1] value\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "print(\"IRIS DATASET\")\n",
    "print(\"*\" * 100)\n",
    "print(\"Total number of rows in Iris dataset = \", iris.data.shape[0])\n",
    "print(\"Total number of columns in Iris dataset = \", iris.data.shape[1])\n",
    "print(\"*\" * 100)\n",
    "print(\"Total number of rows in Features Train dataset = \", iris_features_train.data.shape[0])\n",
    "print(\"Total number of columns in Features Train dataset = \", iris_features_train.data.shape[1])\n",
    "print(\"*\" * 100)\n",
    "print(\"Total number of rows in Features Test dataset = \", iris_features_test.data.shape[0])\n",
    "print(\"Total number of columns in Features Test dataset = \", iris_features_test.data.shape[1])\n",
    "print(\"*\" * 100)\n",
    "print(\"Sum of rows Features Test and Train dataset = \", iris_features_train.data.shape[0] + iris_features_test.data.shape[0])\n",
    "print(\"*\" * 100)\n",
    "print(\"Total number of rows in Target Train dataset = \", iris_response_train.data.shape[0])\n",
    "print(\"*\" * 100)\n",
    "print(\"Total number of rows in Target Test dataset = \", iris_response_test.data.shape[0])\n",
    "print(\"*\" * 100)\n",
    "print(\"Sum of rows Target Test and Train dataset = \", iris_response_train.data.shape[0] + iris_response_test.data.shape[0])\n",
    "print(\"*\" * 100)\n",
    "print(\"The Target Train and Test sets is a vector so there is no .shape[1] value\")\n",
    "print(\"*\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split based on distribution of Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique varibales array = [0 1]\n",
      "Datatype of Unique varibale array = <class 'numpy.ndarray'>\n",
      "Number of unique Target variables = 2\n",
      "Max rows = 24720\n",
      "Min rows = 7841\n",
      "Normal Distribution Rows = 16280.5\n",
      "diff_normal_max = 51.8380885108\n",
      "diff_normal_min = 51.8380885108\n",
      "Is distribution skewed? True\n",
      "Exceuting Skewed Distribution\n"
     ]
    }
   ],
   "source": [
    "### Using the Adult dataset as Iris dataset has equal number of rows for each category (50 each). The below code in its current \n",
    "### form will not go to \"if not skewed_distribution\" logic. \n",
    "# Finding the number of unique values of Target variable in the input dataset.\n",
    "### !!! The target variable name has to be hardcoded !!! ###\n",
    "unique_target_variables = np.unique(adult_data_response)\n",
    "print(\"Unique varibales array =\", unique_target_variables)\n",
    "print(\"Datatype of Unique varibale array =\", type(unique_target_variables))\n",
    "# Finding the number of rows for each of unique value of Target variable in the input dataset\n",
    "number_unique_target_variables = unique_target_variables.shape[0]\n",
    "print(\"Number of unique Target variables =\", number_unique_target_variables)\n",
    "# Finding if the target variable distribution is skewed or not\n",
    "# 1. Find rows of each of unique variable in target dataset\n",
    "row_array = []\n",
    "count_range = range(0, number_unique_target_variables)\n",
    "for count in count_range:\n",
    "    row_array.append(np.where(adult_data_response == unique_target_variables[count])[0].size)\n",
    "# 2. Find minimum and maximum number of rows for target variable\n",
    "max_rows = np.max(row_array)\n",
    "min_rows = np.min(row_array)\n",
    "print(\"Max rows =\", max_rows)\n",
    "print(\"Min rows =\", min_rows)\n",
    "# 3. Find the number of rows that should have have been if the target variable was normally distributed in input dataset\n",
    "normal_distribution_rows = adult_data_response.shape[0] / number_unique_target_variables\n",
    "print(\"Normal Distribution Rows =\", normal_distribution_rows)\n",
    "# 4. If the minimum and maximum number of rows are within 10% of normal distribution then the target variable is normally \n",
    "# distributed in input dataset otherwise the distribution is skewed\n",
    "diff_normal_max = (max_rows - normal_distribution_rows) / normal_distribution_rows * 100\n",
    "diff_normal_min = (normal_distribution_rows - min_rows) / normal_distribution_rows * 100\n",
    "print(\"diff_normal_max =\", diff_normal_max)\n",
    "print(\"diff_normal_min =\", diff_normal_min)\n",
    "\n",
    "if diff_normal_max <= 10 and diff_normal_min <= 10:\n",
    "    skewed_distribution = False\n",
    "else:\n",
    "    skewed_distribution = True\n",
    "print(\"Is distribution skewed?\", skewed_distribution)\n",
    "# If the target variable distribution is not skewed then simply split the input dataset based on %\n",
    "if not skewed_distribution:\n",
    "    print(\"Exceuting Not Skewed Distribution\")\n",
    "    adult_data_features_train, adult_data_features_test, adult_data_response_train, adult_data_response_test = \\\n",
    "            train_test_split(adult_data_features, adult_data_response, test_size = 0.2, random_state = 4)\n",
    "# If the target variable distribution is skewed then \n",
    "else:\n",
    "    print(\"Exceuting Skewed Distribution\")\n",
    "    # Creating the final test and train datasets\n",
    "    # Features will be a dataframe as it is multiple columns\n",
    "    final_train_features = pd.DataFrame({})\n",
    "    final_test_features = pd.DataFrame({})\n",
    "    # Response will be a Series as it has only one column\n",
    "    final_train_response = pd.Series([])\n",
    "    final_test_response = pd.Series([])\n",
    "# 1. Split the input dataset into number of datasets equal to the number of unique values of Target variable, one for each\n",
    "# unique value of Target variable\n",
    "#### It is not advisable to have variable variable names in Python where the variable names themselves are created in a loop. \n",
    "#### We should use a disctianry instead. In the current situation at hand we never know how many unique Target values are in the\n",
    "#### data so creating different dynamically created variable names will get messy. So I am going to work with a temporary\n",
    "#### variable and keep on adding the data to the final test and train variables. This is similar to what I have done in R. See\n",
    "#### https://github.com/divijsharma/RScripts/blob/master/Automated%20-%20Split%20-%20Target%20Variable%20Distribution%20function.R\n",
    "# The target variable will be different in different datasets so the target variable name has to be hardcoded.\n",
    "    for count in count_range:\n",
    "        temp_split = adult_data[adult_data.salary == unique_target_variables[count]]\n",
    "        temp_split_features = temp_split.iloc[:, 0:number_cols-1] # The number of columns is same as raw data. \n",
    "        temp_split_response = temp_split['salary'] # The target variable is same as raw data.\n",
    "\n",
    "# 2. Split each of the above datasets into train and test dataset based on %. Training set will have 80% of the data and test set \n",
    "# will have 20% of the data. This is right now hardcoded. \n",
    "        temp_split_features_train, temp_split_features_test, temp_split_response_train, temp_split_response_test = \\\n",
    "            train_test_split(temp_split_features, temp_split_response, test_size = 0.2, random_state = 4)\n",
    "\n",
    "# 3. Join the various test and train datasets to create the final test and train dataset that will be input to model.\n",
    "        # features will form a dataframe\n",
    "        final_train_features = final_train_features.append(temp_split_features_train)\n",
    "        final_test_features = final_test_features.append(temp_split_features_test)\n",
    "        # Response will form a vector\n",
    "        final_train_response = final_train_response.append(temp_split_response_train)\n",
    "        final_test_response = final_test_response.append(temp_split_response_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in final_train_features = 26048\n",
      "Rows in final_test_features = 6513\n",
      "Rows in final_train_response = 26048\n",
      "Rows in final_test_response = 6513\n"
     ]
    }
   ],
   "source": [
    "print(\"Rows in final_train_features =\", final_train_features.shape[0])\n",
    "print(\"Rows in final_test_features =\", final_test_features.shape[0])\n",
    "print(\"Rows in final_train_response =\", final_train_response.shape[0])\n",
    "print(\"Rows in final_test_response =\", final_test_response.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to split based on Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset_on_target(df, target_col_name, test_size_inp):\n",
    "    number_cols = df.shape[1]\n",
    "    response_df = df[target_col_name]\n",
    "    features_df = df.drop(target_col_name, axis=1)\n",
    "    unique_target_variables = np.unique(response_df)\n",
    "    # Finding the number of rows for each of unique value of Target variable in the input dataset\n",
    "    number_unique_target_variables = unique_target_variables.shape[0]\n",
    "    # Finding if the target variable distribution is skewed or not\n",
    "    # 1. Find rows of each of unique variable in target dataset\n",
    "    row_array = []\n",
    "    count_range = range(0, number_unique_target_variables)\n",
    "    for count in count_range:\n",
    "        row_array.append(np.where(response_df == unique_target_variables[count])[0].size)\n",
    "    # 2. Find minimum and maximum number of rows for target variable\n",
    "    max_rows = np.max(row_array)\n",
    "    min_rows = np.min(row_array)\n",
    "    # 3. Find the number of rows that should have have been if the target variable was normally distributed in input dataset\n",
    "    normal_distribution_rows = response_df.shape[0] / number_unique_target_variables\n",
    "    # 4. If the minimum and maximum number of rows are within 10% of normal distribution then the target variable is normally \n",
    "    # distributed in input dataset otherwise the distribution is skewed\n",
    "    diff_normal_max = (max_rows - normal_distribution_rows) / normal_distribution_rows * 100\n",
    "    diff_normal_min = (normal_distribution_rows - min_rows) / normal_distribution_rows * 100\n",
    "\n",
    "    if diff_normal_max <= 10 and diff_normal_min <= 10:\n",
    "        skewed_distribution = False\n",
    "    else:\n",
    "        skewed_distribution = True\n",
    "    # If the target variable distribution is not skewed then simply split the input dataset based on %\n",
    "    if not skewed_distribution:\n",
    "        features_train, features_test, response_train, response_test = \\\n",
    "            train_test_split(features_df, response_df, test_size = test_size_inp, random_state = 4)\n",
    "    # If the target variable distribution is skewed then \n",
    "    else:\n",
    "        # Creating the final test and train datasets\n",
    "        # Features will be a dataframe as it is multiple columns\n",
    "        features_train = pd.DataFrame({})\n",
    "        features_test = pd.DataFrame({})\n",
    "        # Response will be a Series as it has only one column\n",
    "        response_train = pd.Series([])\n",
    "        response_test = pd.Series([])\n",
    "    # 1. Split the input dataset into number of datasets equal to the number of unique values of Target variable, one for each\n",
    "    # unique value of Target variable\n",
    "    #### It is not advisable to have variable variable names in Python where the variable names themselves are created in a loop. \n",
    "    #### We should use a disctionary instead. In the current situation at hand we never know how many unique Target values are in the\n",
    "    #### data so creating different dynamically created variable names will get messy. So I am going to work with a temporary\n",
    "    #### variable and keep on adding the data to the final test and train variables. This is similar to what I have done in R. See\n",
    "    #### https://github.com/divijsharma/RScripts/blob/master/Automated%20-%20Split%20-%20Target%20Variable%20Distribution%20function.R\n",
    "    # The target variable will be different in different datasets so the target variable name has to be hardcoded.\n",
    "        for count in count_range:\n",
    "            temp_split = df[df[target_col_name] == unique_target_variables[count]]\n",
    "            temp_split_features = temp_split.drop(target_col_name, axis=1) # The number of columns is same as raw data. \n",
    "            temp_split_response = temp_split[target_col_name] # The target variable is same as raw data.\n",
    "\n",
    "    # 2. Split each of the above datasets into train and test dataset based on %. Training set will have 80% of the data and test set \n",
    "    # will have 20% of the data. This is right now hardcoded. \n",
    "            temp_split_features_train, temp_split_features_test, temp_split_response_train, temp_split_response_test = \\\n",
    "                train_test_split(temp_split_features, temp_split_response, test_size = test_size_inp, random_state = 4)\n",
    "\n",
    "    # 3. Join the various test and train datasets to create the final test and train dataset that will be input to model.\n",
    "    # features will form a dataframe\n",
    "            features_train = features_train.append(temp_split_features_train)\n",
    "            features_test = features_test.append(temp_split_features_test)\n",
    "        # Response will form a vector\n",
    "            response_train = response_train.append(temp_split_response_train)\n",
    "            response_test = response_test.append(temp_split_response_test)\n",
    "            \n",
    "    # Return all the datasets\n",
    "    return features_train, features_test, response_train, response_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of how to call function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train_features_1, app_train_features_2, app_train_response_1, app_train_response_2 =  \\\n",
    "    split_dataset_on_target(app_train, 'TARGET', 0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
